#!/usr/bin/env node

const { exec } = require('child_process');
const path = require('path');
const fs = require('fs');
const util = require('util');
const os = require('os');
const tar = require('tar');

// åœ¨æ‰€æœ‰å…¶ä»–ä»£ç¢¼ä¹‹å‰åŠ è¼‰ç’°å¢ƒè®Šæ•¸
require('dotenv').config({ path: path.resolve(__dirname, '..', '.env.local') });

const execAsync = util.promisify(exec);

// --- å¾ç’°å¢ƒè®Šæ•¸è§£æè³‡æ–™åº«è¨­å®š ---
const MONGODB_URI = process.env.MONGODB_URI;

if (!MONGODB_URI) {
  console.error('\x1b[31mâŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° MONGODB_URI ç’°å¢ƒè®Šæ•¸ã€‚\x1b[0m');
  console.error('\x1b[33mè«‹ç¢ºä¿åœ¨ /next ç›®éŒ„ä¸‹æœ‰åç‚º .env.local çš„æª”æ¡ˆï¼Œä¸”å…¶ä¸­åŒ…å« MONGODB_URI çš„è¨­å®šã€‚\x1b[0m');
  process.exit(1);
}

let DB_NAME, DB_USER, DB_PASS, DB_HOST, DB_PORT;
try {
  const uri = new URL(MONGODB_URI);
  DB_NAME = uri.pathname.substring(1); // ç§»é™¤é–‹é ­çš„ '/'
  DB_USER = uri.username;
  DB_PASS = uri.password;
  DB_HOST = uri.hostname;
  DB_PORT = uri.port;
} catch (error) {
  console.error('\x1b[31mâŒ éŒ¯èª¤ï¼šMONGODB_URI æ ¼å¼ä¸æ­£ç¢ºã€‚\x1b[0m');
  console.error(error);
  process.exit(1);
}

// --- å‚™ä»½è¨­å®š ---
const BACKUP_DIR = path.join(__dirname, '..', 'db', 'backups');
const DOCKER_CONTAINER_NAME = 'mongo';

const COLLECTIONS_CONFIG = {
  core: ['companies', 'tenders', 'ai_tools', 'feedbacks'],
  cache: ['pcc_api_cache', 'g0v_company_api_cache', 'twincn_api_cache'],
};

// é¡è‰²è¼¸å‡º
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  green: '\x1b[32m',
  blue: '\x1b[34m',
  yellow: '\x1b[33m',
  cyan: '\x1b[36m',
  red: '\x1b[31m',
};

function colorize(text, color) {
  return `${colors[color]}${text}${colors.reset}`;
}

// --- è¼”åŠ©å‡½å¼ ---
function getCollectionsToBackup(scope) {
  switch (scope) {
    case 'core':
      return COLLECTIONS_CONFIG.core;
    case 'all':
    default:
      return [...COLLECTIONS_CONFIG.core, ...COLLECTIONS_CONFIG.cache];
  }
}

async function checkDockerContainer() {
  try {
    const { stdout } = await execAsync(
      `docker ps --filter "name=${DOCKER_CONTAINER_NAME}" --format "{{.Names}}"`
    );
    if (!stdout.trim().includes(DOCKER_CONTAINER_NAME)) {
      throw new Error(
        `MongoDB å®¹å™¨ '${DOCKER_CONTAINER_NAME}' æœªé‹è¡Œï¼è«‹å…ˆå•Ÿå‹• Docker æœå‹™ã€‚`
      );
    }
  } catch (error) {
    throw new Error(`Docker å®¹å™¨æª¢æŸ¥å¤±æ•—ï¼š${error.message}`);
  }
}

async function main() {
  console.log(colorize('\nğŸ”„ MongoDB å…¨é¢å‚™ä»½å·¥å…·', 'bright'));
  console.log(colorize('='.repeat(50), 'cyan'));

  const args = process.argv.slice(2);
  const scopeArg = args.find(arg => arg.startsWith('--scope='));
  const scope = scopeArg ? scopeArg.split('=')[1] : 'all';

  let tempDir;
  try {
    // 1. æª¢æŸ¥ Docker å®¹å™¨
    await checkDockerContainer();
    console.log(colorize('âœ… Docker å®¹å™¨æª¢æŸ¥é€šé', 'green'));

    // 2. æ±ºå®šè¦å‚™ä»½çš„ Collections
    const collectionsToBackup = getCollectionsToBackup(scope);
    if (collectionsToBackup.length === 0) {
      console.log(colorize('ğŸ¤” ç„¡éœ€å‚™ä»½çš„ Collections', 'yellow'));
      return;
    }
    console.log(
      colorize(`ğŸ¯ å‚™ä»½ç¯„åœ: ${scope}`, 'blue') +
        ` (${collectionsToBackup.length} å€‹é›†åˆ)`
    );

    // 3. å»ºç«‹è‡¨æ™‚ç›®éŒ„
    tempDir = fs.mkdtempSync(path.join(os.tmpdir(), 'mongodb-backup-'));
    console.log(colorize(`ğŸ“‚ å»ºç«‹è‡¨æ™‚ç›®éŒ„: ${tempDir}`, 'blue'));

    // 4. é€ä¸€åŒ¯å‡º Collection
    for (const collection of collectionsToBackup) {
      console.log(
        `  -> æ­£åœ¨åŒ¯å‡º ${colorize(collection, 'yellow')}...`
      );
      const output_file = path.join(tempDir, `${collection}.json`);
      const command = [
        `docker exec ${DOCKER_CONTAINER_NAME}`,
        'mongoexport',
        `--host=${DB_HOST}:${DB_PORT}`,
        `--db=${DB_NAME}`,
        `--collection=${collection}`,
        `--username=${DB_USER}`,
        `--password=${DB_PASS}`,
        '--authenticationDatabase=admin',
        '--jsonArray',
        '--pretty',
        `--out=/tmp/${collection}.json`,
      ].join(' ');

      await execAsync(command);

      // å¾ Docker å®¹å™¨ä¸­è¤‡è£½å‡ºä¾†
      await execAsync(
        `docker cp ${DOCKER_CONTAINER_NAME}:/tmp/${collection}.json "${output_file}"`
      );

      // æ¸…ç†å®¹å™¨å…§çš„è‡¨æ™‚æª”æ¡ˆ
      await execAsync(
        `docker exec ${DOCKER_CONTAINER_NAME} rm /tmp/${collection}.json`
      );
    }
    console.log(colorize('âœ… æ‰€æœ‰ Collections åŒ¯å‡ºå®Œæˆ', 'green'));

    // 5. å£“ç¸®æˆ .tar.gz
    if (!fs.existsSync(BACKUP_DIR)) {
      fs.mkdirSync(BACKUP_DIR, { recursive: true });
    }
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const archiveName = `db-backup-${scope}-${timestamp}.tar.gz`;
    const archivePath = path.join(BACKUP_DIR, archiveName);

    console.log(colorize(`ğŸ“¦ æ­£åœ¨å£“ç¸®æª”æ¡ˆ... ${archiveName}`, 'blue'));
    await tar.c(
      {
        gzip: true,
        file: archivePath,
        cwd: tempDir,
      },
      fs.readdirSync(tempDir)
    );

    console.log(colorize('\nğŸ‰ å‚™ä»½æˆåŠŸå®Œæˆï¼', 'bright'));
    console.log(colorize(`ğŸ“„ å‚™ä»½æª”æ¡ˆ: ${archivePath}`, 'green'));
  } catch (error) {
    console.error(colorize(`\nâŒ å‚™ä»½å¤±æ•—: ${error.message}`, 'red'));
    process.exit(1);
  } finally {
    // 6. æ¸…ç†è‡¨æ™‚ç›®éŒ„
    if (tempDir && fs.existsSync(tempDir)) {
      fs.rm(tempDir, { recursive: true, force: true }, () => {
        console.log(colorize('ğŸ§¹ è‡¨æ™‚æª”æ¡ˆæ¸…ç†å®Œæˆ', 'blue'));
      });
    }
  }
}

// ç¢ºä¿æœ‰å®‰è£ tar
async function checkDependencies() {
  try {
    require.resolve('tar');
  } catch (e) {
    console.error(colorize('âŒ ç¼ºå°‘ `tar` ä¾è³´å¥—ä»¶ã€‚', 'red'));
    console.log(colorize('è«‹åŸ·è¡Œ `npm install tar` æˆ– `yarn add tar`', 'yellow'));
    process.exit(1);
  }
}

checkDependencies().then(main);